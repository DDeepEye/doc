<< 앞으로의 계획 >>
	- 딥러닝 자연어 처리 중급을 일주일안에 학습한다.
	- 딥러닝 강화 학습을 심도있게 본다.
	- 오픈소스 코드 게임을 찾아서 강화 학습 또는 내가 만든 딥러닝 알고리즘으로 ai를 제작해 본다.


<< 딥러닝 학습 노트>>
	- 표준편차 ? 정규분포에 대해서 알아보자
		- 유닛 가우시안 에 대해서 알아보기
		- 가우시안 분포란?
	- pandas ,seaborn ,matplotlib ,sklearn 등의 파이썬 모듈의 개요에 대해서 알아 보자

	- Regularizations
		* 흠.. 이파트는 좀 설명이 약한것 같다.
		다른 자료를 좀 찾아서 보충을 해야 할것 같다.

		- vaildate loss 로 하이퍼 파라미터를 조절했다고?
		이상한데.. 그건 그냥 adamoptim 에 맡겼던것 같은데 이 부분 확인해 보자
			

		- vscode로 파이토치 환경을 구성
			- 결과를 보여주는 pairplot 과 딥러닝을 구분하는 코드를 짜자


	- vscode 에서 기초부터 만들어 보기로 하자
		연습이 많이 필요하다 아마도 파이썬 문법이나 이런것도 확인해봐야 하는것들이 있을것이다.
		한줄한줄 디버깅으로 실행하면서 내부적으로 어떤일들을 진행하는지 보기로 하자


	- BatchNorm 의 개념 다시 복습하기
		- active layer 도 다시 한번 보기로 하자 (ReLu, LeakyReLU)
	- Dence Vector 가 유사도 체크가 가능하기 때문에 게임에 행동 ai 에 더 좋은 영향을 줄거 같다.
	- 오토인코더에 대한 다른 자료를 한번 찾아 보기로 하자
	- 확률과 통계 개념을 이해해야 할것 같다.
		- 이항확률분포
			- 데이타를 어떻게 확률변수, 평균, 분산, 표준편차 로 나타냈는지 다시 한번 보기로 하자
		- 정규분포
			- 계산 공식을 일단 외우기로 하고 필요하다면 대학수학의 유도 과정을 찾아 보기로 하자.

	- 데이타 사이언스의 수학 공식들을 설명한 부분을 다시 한번 하나하나 수식을 전개 해 보도록하자.

	- 선형수학을 틈틈히 공부할때가 된것 같다.
		- 벡터부터 시작하자 차근 차근..

	@ 백터, 행렬, 확률과 통계가 핵심이다.
	그냥 이번주는 차근 차근히 볼수 밖에 없다.
	별다른 수가 없다. 
	대학원 다니고 박사 과정 밟던 놈들이 다루던 문제들이다.
	나도 그들에 준하는 지식을 가져야 한다.
	일주일 에서 한달이면 그들에 비하면 오히려 거저먹는 수준일것이다.

	** 데일리 학습 노트
	- 각 레이어의 행렬 계산시 데이타를 어떻게 채우는지 다시 확인할것
	- batchnorm 에 대한 복습


	** 6월 13일 데일리 학습노트
		- convolusion layer 를 이용해서 엔코딩과 디코딩을 만들어 보고
		레이어를 작게 쌓으면서 점점 층을 높여서 쌓았을때 그림이 어떻게 변해 가는지 확인해 보도록 하자.
		- rnn 에서 히든 사이즈에 대해서 명확하게 짚고 넘어가 보자
		- 복습 리스트
			- back propagation 에 대해서 복습하자
			- adam 다시 한번 복습

		- 조건부 확률의 적분
			- 확률과 통계 대학 과정 유튜브 자료는 봐야 한다.. 어쩔수가 없다.

	** 6월 21일 데일리 학습노트
		- for c in words[i - w_size:i + w_size]: #파이썬에서 이 구문이 문제가 안된다고!!?? 일단 챕터5를 다 끝내고 확인을 한번 해보기로 하자
		- Embedding Layer에 어떤 작업을 하는지 확실히 봐두도록하자.

	** 6월 24일 데일리 학습노트
		- 데이터 공정 다시 보고 직접 수행해보기
			- 설치할게 필요하면 설치 하면서 진행하자
		- embedding layer 란?
			@ 사실은 그냥 리니어 레이어에 불과 함
			단어 하나당 한 low에 대응된다. 만약 단어가 5000 단어랑 임베딩 레이어는 (5000, output)
		- text classification 실습 직접 해보기
		- rnn 구조 다시 한번 확인하고 hidden_size 의 의미 다시 확인하기
		- cnn "윈도우 사이즈" 와 "필터" 다시 확인해보기

	** 7월 3일 데일리 학습노트
		- 행렬 연산을 할때 battch_size 예전에 어떻게 연산해서 처리 했었는지 다시 확인할것
			@ 예전 딥러닝 유치원 과정에서 행렬 연산과 mini battch를 어떻게 처리했었는지를 다시 봐야 한다.
		- torchtext 실습 다시 한번 확인
		- text classification 실제 감정 처리 부분 확인

		- cnn "윈도우 사이즈" 와 "필터" 다시 확인해보기
		- classifi.py 에서 마지막에 exp()함수로 로그 확률값으로 확인을 해야 하는데.. loss 값 계산을 logsoftmax 로 했기 때문인듯.. logsoftmax 로 loss 값 구하는 부분 다시 복습하자
		- classifi.py 를 수정해서 'mecab -O wakati'의 처리가 스크립트 안쪽에서 처리 되도록 바꿔 보자 

	** 7월 5일 데일리 학습노트
		- classifi.py 에서 마지막에 exp()함수로 로그 확률값으로 확인을 해야 하는데.. loss 값 계산을 logsoftmax 로 했기 때문인듯.. logsoftmax 로 loss 값 구하는 부분 다시 복습하자
		- LSTM 의 input/output hidden state tensor 다시 복습하기
		- 왜 자연상수e 를 사용할까?
			@ 미분 계산이 빠르고 편하다.

	** 7월 14일 데일리 학습노트
		- 챕터4를 한번 학습한 다음에는 소스를 찬차히 분석해 보자
			- NLLoss 를 사용할때 파라메터 에 대해서 토큐먼트를 참조하자
				@ NLLoss 에 입력되는 y_hat의 각 요소들은 (1 > y_hat > 0) log(x)이기 때문에 항상 음수값 을 가진다.  
			- !! continue train 은 일단 한번 들어 보고 조만간에 컴퓨터를 새로 사면 한번 해보도록 하자.
				- train 과 continue train 이 분리 되어 있는데 이것을 하나로 통합하고 config 파일을 참조해서 어떤 방향으로 train을 할지 결정하도록 하자.
			- ppl 이 뭐였더라?
			- argparse.ArgumentParser 기능을 config.txt 파일을 읽어 들여서 처리 하도록 바꾸자 콘솔창에다가 일일이 config 데이타를 타이핑하는것은 굉장히 비하면효율적이다.
			- 각 레이어의 사이즈들이 어떻게 대응되는지 명확하게 짚고 넘어가자 특히 attention과 generator 벡터 사이즈 면밀히 살펴 보도록 하자.
			- contiguous 함수를 쓰는 이유는 알았다 그렇다면 행렬 연산으로 데이타의 형태가 어떻게 바뀌는지 눈여겨 봐야 보고 머릿속에 그려 보도록 하자.
			- validate 함수의 y_hat 사이즈 가 (batch_size, length, output_size)일거 같은데 다르게 표시 되어 있다 이거 꼭 확인해 보자
		- rnn 의 cell state 에 대해서 다시 확인해보자
			- hidden state 는 정확하게 어떤 의미를 가질까?
		- torchtext 도큐먼트를 좀 참조하도록 하자.
		- DataLoader의 맴버변수 src, tgt 가 같은 단어 수집을 한것으로 보여지는데 pc를 새로 사게 되면 데이타를 직접 손보면서 확인해보도록 하자.
		- 새로운 컴퓨터를 맞추지 못하면 학습은 불가능하다.


		1. 이전에 다루던 loss 와 자연어 생성에서의 loss 는 어떤 차이가 있는가?
			@ 사실 loss 확률을 모사하는것 확률들을 합한것이 가장 클때 이경우 그러니까 이 경우에는 log값이기 때문에 항상 y_hat의 각 값 예를 들면 y_hat[:,:] < 0 이다.
			모든 battch 의 각 타임 라인의 확률 log 값을 모두 합친 값이 클수록 잘 모사하고 있다고 말할수 있다.

			- rnn 에서 pad는 어떻게 계산되는가?
				NLLoss 생성할때 target 단어 리스트에서 pad에 해당하는 인덱스는 1이다 그 인덱스에 가중치를 0으로 주고 나머지는 전부 1로 줘서 실제로 loss 값을 구할때 pad에 해당하는 one_hot 은 전부 0으로 만들어 준다.

		2. 문장 한줄당 각 단어의 확률 값에 log 를 취했다면 그 줄의 모든 단어 확률의 log 값을 모두 합한다면 어떤 값이 나올까?
			@ 위에 1번에서 설명 되었음

		3. ppl 이 무엇이지?
			@ loss 값은 0에 가까울수록 좋지만 ppl은 무조건 작은 값일수록 좋다.
			공식과 의미를 다시 한번 되짚어 보는 시간을 가져 보자

		4. validate 함수의 y_hat 사이즈 가 (batch_size, length, output_size)일거 같은데 다르게 표시 되어 있다 이거 꼭 확인해 보자
			@ 주석문이 잘못 되었다고 보여진다.
			validate 건 train 이건 y_hat의 shape 사이즈는 동일할것이다.
			하지만 시스템 맞추고 나서 실습할때 한번 테스트를 해보도록 하자

		5. rnn의 cell state 는 어떤 의미를 가지는가?
			행렬 연산의 과정을 유심히 보고 정보가 어떻게 대응되며 압축 되는지 면밀히 검토하자
			@ 이전 단어들의 정보를 얼마나 가져올지 판단하는 state

		6. hidden state 는 정확하게 어떤 의미를 가지는가?
			행렬 연산의 과정을 유심히 보고 정보가 어떻게 대응되며 압축 되는지 면밀히 검토하자
			- encoder 와 decoder 의 데이터 처리가 실제 코드에서 어떻게 적용 되고 있는지 확인하도록 하자.

		7. contiguou , transpose 그리고 텐서 슬라이싱 실습하도록 하자

		8. 패스트캠퍼스에서 보낸 코드가 annotation을 달다 보니 맞지 않는게 보인다. 
			제대로 돌수 있도록  타입을 제대로 맞춰 주자	

		9. EOS 처리가 정말 이상하다 pc 새로 맞춰서 실습할때 진짜 eos 가 다 끝에만 있는지 확인해 보자
			@ 이메일로 답변 받음.. eos 는 없지만 어차피 마지막은 pad 아니면 eos 이므로 하나를 빼서 weight 를 빼는 효과를 줄수도 있다고 함.. 
			그보다는 디코더에는 eos를 넣어주는게.. teacher forcing 에서는 더 효과가 좋아 보이는데..

		10. 학습이 끝나면 빅데이타를 받기 위해서 가입을 하도록 하자
			- ai hub 에 가입하기

		11. batchsize = 1 을 넣고 break point 로 데이타랑 확인해 볼수 있는 코드를 짜서 보도록 하자.

		12. beam_search 의 구조를 좀 나눠 보도록 하자.

		13. tensor 연산 다시 복습하도록 하자. 한번 보면 알수 있을 정도로 익숙해져야 한다.

		14. batch_beam_search 함수는 pc를 새로 맞추게 되면 내부적으로 한줄 한줄 어떻게 처리 되고 있는지 내부 데이타와 구조의 변화를 파악할 필요가 있어 보인다.

		15. 기존 rnn 과 battch_beam_search 의 차이를 확실하게 이해하고 넘어 가도록 하자.

		16. 텐서를 어떻게 병렬 처리 하는지에 대해서 익숙해질 필요가 있다.

		17. pantuml 을 이용해서 각 model의 train 과 valid 를 다이어그램으로 그려 보도록 하자
			- 딥러닝 기초 부터 하나씩 그리면서 추가해보도록 하자.

		18. 파이썬에서 *는 포인터와 비슷한 의미인가?

		19. weight 값의 행렬 연산으로 전달될때 그 행렬값에는 어떤 데이타들의이 포함 영향을 주는지 데이타의 흐름을 잘 보도록 하자.
			- dropout 다시 복습해야함
			- transformer 에서 encoding block 과 decoding block 의 multi attention 처리를 유심히 볼 필요가 있다.
			- transformer 에서 각 mask 들을 어떻게 이용하는지 다시 복습하자
			- sentence embedding matrix 가 필요한 이유는 어렴풋이 알겠는데 그 안에 sin , cos  연산과 그게 실제 데이타에 어떻게 영향을 미치는지 의미를 제대로 파악해보도록 하자 
			- transformer 에서 position encoding 처리 복습


		20. tensor shape 를 변경하는 함수들의 예제 코드들을 쥬피터 노트북을 이용해서 직접 연습하도록하자.
			- 각 연산을 하는 함수들도 주요 함수들도 예제 코드들을 짜서 연습하도록 해보자
			- 텐서 broad casting 연산에 대해서 익숙지도록 예제 코드를 짜자

		21. with torch.no_grad() 때 생성되는 텐서의 경우 학습을 시키지 않도록 한다는데.. 모든 텐서는 생성될때 grad 에 타겟이 된다는 뜻인가?

		22. rnn 의 gradiant clipping 이 뭐였지? sgd 에서 쓰인다는데..



		pc를 새로 맞추면 해야할 사항들

			4. validate 함수의 y_hat 사이즈 가 (batch_size, length, output_size)일거 같은데 다르게 표시 되어 있다 이거 꼭 확인해 보자
				@ 주석문이 잘못 되었다고 보여진다.
				validate 건 train 이건 y_hat의 shape 사이즈는 동일할것이다.
				하지만 시스템 맞추고 나서 실습할때 한번 테스트를 해보도록 하자

			10. 학습이 끝나면 빅데이타를 받기 위해서 가입을 하도록 하자
			- ai hub 에 가입하기

			11. batchsize = 1 을 넣고 break point 로 데이타랑 확인해 볼수 있는 코드를 짜서 보도록 하자.

			12. beam_search 의 구조를 좀 나눠 보도록 하자.
				- bos를 5개로 병렬로 넣는데.. 각각 다른 확률이 나올수가 있다고? 그게 좀 이상하다.
				굉장히 작은 샘플로 실험을 해보도록 하자
				- 각각의 확률을 가중치로 누적하는 코드를 보지 못했다 그부분이 어디인지 확인해 보자

			14. batch_beam_search 함수는 pc를 새로 맞추게 되면 내부적으로 한줄 한줄 어떻게 처리 되고 있는지 내부 데이타와 구조의 변화를 파악할 필요가 있어 보인다.

			15. 대분류에서 소분류로 가기 전에 대분류에서 새로운 label이 필요할수도 있다 그러기 위해서 게임 ai 에서는 새로운 패턴을 인지하고 대분류를 자동으로 늘려줄수도 있을까?


		config 내용을 따로 텍스트 파일로 빼도록 하자 현재는 너무 보기 안좋다.


	* 복습 리스트 (일단 정주행을 끝까지 하도록 하자.)
		1. pytorch 의 텐서 계산과 shape 처리에 복습

		2. 각 모델과 이론에 대한 복습
			- 각 모델들의 데이타가 어떤 의미를 가지며 각각의 계층들이 어떤 의미로 데이터를 다루는지 명확하게 알고 가도록 하자.
				- plantuml 로 기초적인 구조부터 하나씩 그려 가면서 해보도록 하자.
					- 특히 각 레이어가 텐서를 어떻게 계산하고 결과 값을 내는지 유심히 보도록 하자.
			- 수식들이 어떻게 코드로 구현되는지 확인하고 직접 코드로 수식을 짜보도록 하자
			- 각각의 모델들의 처리의 차이점을 비교하면서 한층 더 깊이 이해해보자

		3. 워크스테이션을 구입하게 되면 해야 할 사항들
			- 작업 환경 만들기 (vsstudio , vscode, python, pytorch, plantmul, 딥러닝에 필요한 플러그인 또는 라이브러리, git)
			- ai hub 에 등록해서 데이타 받기
			- plantuml 을 활용한 다이어그램을 그려보기
			- 기초적인 모델부터 차근 차근 다시 만들어 보기
			- 작은 샘플로 실제 학습 중간 중간 데이타가 내가 생각한 대로 흘러 가는지 확인하기
			- 본격적으로 데이타를 이용해서 트렌스포머를 학습시켜 보기
				- 학습된 모델을 가지고 테스트를 한번 해보기

			- 언리얼이나 유니티를 설치해서 강화 학습을 시킬수 있는 컨텐츠를 찾아 보기
				- 내가 생각한 데이터 feature 와 모델로 실제 학습 시켜보기


 


